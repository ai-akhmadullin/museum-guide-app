\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this thesis, we explored, implemented, and evaluated a mobile application capable of recognizing museum exhibits in real time. The primary objective was to provide museum visitors with quick, accurate access to information about exhibits directly through their Android smartphones.

We began by preparing a suitable dataset by recording videos of exhibits and extracting individual frames. Care was taken to ensure balanced representation of each exhibit, extracting frames at regular intervals and subsequently labeling them using metadata stored in a CSV file. This approach allowed us to create a well-structured dataset for our task.

The initial exploration of potential approaches identified two main solutions for achieving accurate image recognition: the embedding-based similarity search and direct classification using transfer learning. The first method, embedding-based similarity search, although highly flexible, was soon discarded due to practical limitations, notably the high computational requirements and significant storage overhead of embeddings on mobile devices. Alternatively, direct classification through transfer learning proved to be the most suitable option, balancing accuracy, performance, ease of integration, and storage efficiency.

For the classification architecture, MobileNetV2 was chosen due to its compact and efficient design aimed at mobile environments. Through experimentation with different configurations --- such as batch sizes, number of layers, dropout rates, and fine-tuning --- we arrived at a final optimized model that achieved validation accuracy of approximately 95.9\%.

An analysis of misclassifications revealed that errors mostly occurred on visually similar exhibits or when multiple objects appeared within a single image. To mitigate this, the application's user interface includes not only the model's top prediction but also several alternative suggestions (in a non-intrusive manner). This allows users to easily access information about exhibits, even when predictions are not entirely accurate.

The developed Android application integrates TensorFlow Lite for lightweight on-device inference and employs CameraX for simple, efficient camera access. The application includes multiple screens --- from a home and gallery view to dedicated exhibit detail and real-time scanner screens --- organized clearly and intuitively. Additionally, a debug mode was implemented for convenient monitoring and adjusting classification thresholds and inference time evaluation.

Real-world testing was conducted at the Museum of Decorative Arts in Prague (UPM). Practical evaluation on a subset of 80 museum exhibits yielded an accuracy of around 93.75\%, closely matching expectations derived from validation results. Misclassifications again typically involved visually similar exhibits, yet the correct exhibit usually appeared within the offered alternative suggestions. Furthermore, inference was fast enough --- around 120--150 ms --- to provide smooth, responsive real-time interaction.

Nevertheless, there are several practical ways to build upon this work. Expanding the dataset is always beneficial, as training on more images under various conditions generally helps models achieve better generalization. Additionally, future work could explore alternative neural network architectures apart from MobileNetV2; examples include EfficientNet Lite or ShuffleNet, both specifically designed to run efficiently on mobile devices while maintaining good classification accuracy. It could also be beneficial to experiment with entirely different approaches, including object detection techniques such as YOLO or SSD. Object detection would enable simultaneous localization and recognition of multiple exhibits within a single camera frame, directly addressing one of the challenges identified during evaluation. Furthermore, hybrid approaches that combine CNN-based models with classical image descriptors or utilize additional sensor data (e.g., Bluetooth beacons or NFC tags) could offer increased robustness and improved recognition performance in challenging scenarios.

Overall, this work successfully demonstrated that modern machine learning methods in combination with optimized model architectures and mobile technologies can effectively support real-time museum exhibit recognition. The resulting application not only performed reliably within a controlled evaluation but also showed potential applicability to broader museum contexts. With incremental improvements and adaptations, such tools could meaningfully assist visitors, providing convenient, immediate, and easy-to-access information right at their fingertips.
